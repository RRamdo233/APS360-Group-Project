{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import torch.nn.init as init\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_list = next(os.walk('../Data/Images'))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/Labels.csv\"\n",
    "header = ['Run','AD']\n",
    "Labelsdf = pd.read_csv(path, names=header, usecols=[1,2], skiprows=1, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = dict(zip(Labelsdf.Run, Labelsdf.AD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# process then save the 3d image ndarray as binary\n",
    "# Images of 96, 96, 48 from 256, 176, 256\n",
    "\n",
    "# Crop in the x and y direction (get center/hippocampi area)\n",
    "# [80:176, 40:136, ]\n",
    "\n",
    "img_dir = \"../Data/Images\"\n",
    "dest_dir = \"../Data/Processed\"\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "    print(\"Created ouput directory: \" + dest_dir)\n",
    "\n",
    "    depth_scale = 2\n",
    "\n",
    "    imgs, labels = [], []\n",
    "\n",
    "    for run in runs_list:\n",
    "        run_dir = os.path.join(img_dir, run)\n",
    "        run_imgs = []\n",
    "        for filename in os.listdir(run_dir):\n",
    "            img_slice = cv2.imread(os.path.join(run_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            # if image is square? IF DO THIS, NEED TO ACCOUNT FOR CENTER OFF SET (176 X 240)\n",
    "            if  img_slice is None or img_slice.shape[0] == img_slice.shape[1]:\n",
    "                print(run)\n",
    "                print('1')\n",
    "                break\n",
    "            img_num = int(filename[-7:-4])\n",
    "            if (100 <= img_num < 196):\n",
    "                img_slice = cv2.imread(os.path.join(run_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "                #img_slice = cv2.resize(img_slice, (0,0), fx=1/img_scale, fy=1/img_scale, interpolation=cv2.INTER_AREA)\n",
    "                img_slice = img_slice[80:176,40:136]\n",
    "                # normalize pixel intensity to range of 0 and 1\n",
    "                img_slice = img_slice/256\n",
    "                run_imgs.append(img_slice)\n",
    "\n",
    "        if len(run_imgs) == 0 or len(run_imgs) != 96:\n",
    "            print(run)\n",
    "            print('2')\n",
    "            continue\n",
    "\n",
    "        temp_arr = np.array(run_imgs)\n",
    "\n",
    "        final_slices = []\n",
    "\n",
    "        for y in range(temp_arr.shape[2]):\n",
    "            xz_pane = temp_arr[:, :, y]\n",
    "            scaled_xz = cv2.resize(xz_pane, (0, 0), fy=1/depth_scale, fx=1, interpolation=cv2.INTER_AREA)\n",
    "            final_slices.append(scaled_xz)\n",
    "\n",
    "        if not len(final_slices[0]) == 48:\n",
    "            print(run)\n",
    "            print('3')\n",
    "            continue\n",
    "\n",
    "        final_slices = np.dstack(final_slices)\n",
    "        final_slices = np.expand_dims(final_slices, axis=0)\n",
    "        if run in labels_dict:\n",
    "            # save as binary\n",
    "            np.save(os.path.join(dest_dir, run), final_slices)\n",
    "            imgs.append(run)\n",
    "            labels.append(labels_dict[run])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    imgs = np.array(imgs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    np.save(os.path.join(dest_dir, 'imgs'), imgs)\n",
    "    np.save(os.path.join(dest_dir, 'labels'), labels)\n",
    "\n",
    "            \n",
    "else:\n",
    "    imgs = np.load(os.path.join(dest_dir, 'imgs.npy'))\n",
    "    labels = np.load(os.path.join(dest_dir, 'labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from\n",
    "model_name = \"vgg3d\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, train_labels = [], []\n",
    "val_imgs, val_labels = [], []\n",
    "test_imgs, test_labels = [], []\n",
    "\n",
    "for idx, ad in np.ndenumerate(labels):\n",
    "    if (idx[0] % 10) < 6:\n",
    "        train_imgs.append(imgs[idx[0]])\n",
    "        train_labels.append(ad)\n",
    "    if ((idx[0] % 10) >= 6) and ((idx[0] % 10) < 8):\n",
    "        val_imgs.append(imgs[idx[0]])\n",
    "        val_labels.append(ad)\n",
    "    if ((idx[0] % 10) >= 8) and ((idx[0] % 10) <= 9):\n",
    "        test_imgs.append(imgs[idx[0]])\n",
    "        test_labels.append(ad)\n",
    "        \n",
    "train_imgs = np.array(train_imgs)\n",
    "train_labels = np.array(train_labels)\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_labels = np.array(val_labels)\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_labels = np.array(test_labels)\n",
    "        \n",
    "ad_imgs, ad_labels = [], []\n",
    "\n",
    "for idx, ad in np.ndenumerate(train_labels):\n",
    "    if ad:\n",
    "        ad_imgs.append(imgs[idx[0]])\n",
    "        ad_labels.append(ad)\n",
    "\n",
    "ad_imgs = np.array(ad_imgs)\n",
    "ad_labels = np.array(ad_labels)\n",
    "\n",
    "normal_ratio = train_imgs.shape[0] // ad_imgs.shape[0]\n",
    "print(\"Normal ratio: \" + str(normal_ratio))\n",
    "\n",
    "train_imgs = np.concatenate((train_imgs, np.repeat(ad_imgs, normal_ratio - 1, axis=0)), axis=0)\n",
    "train_labels = np.concatenate((train_labels, np.repeat(ad_labels, normal_ratio -1, axis=0)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T1Dataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = data\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = np.load(os.path.join(dest_dir, self.data[index]) + \".npy\")\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = self.target[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = T1Dataset(train_imgs, train_labels)\n",
    "val_dataset = T1Dataset(val_imgs, val_labels)\n",
    "test_dataset = T1Dataset(test_imgs, test_labels)\n",
    "\n",
    "image_datasets = {'train': train_dataset, 'val': val_dataset, 'test': test_dataset}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                   batch_size=batch_size,\n",
    "                                                   pin_memory=True,\n",
    "                                                   shuffle=True, num_workers=0) for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG3D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGG3D, self).__init__()\n",
    "        \n",
    "        self.features3d = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #256\n",
    "            \n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #128\n",
    "            \n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #64\n",
    "            \n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #32\n",
    "            \n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #16\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 3, 3))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 3 * 3, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 2),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features3d(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            normal_corrects, ad_corrects = 0, 0\n",
    "            normal_count, ad_count = 0, 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                correctness = preds == labels.data\n",
    "                running_corrects += torch.sum(correctness)\n",
    "\n",
    "                index = 0\n",
    "                for condition in labels.data:\n",
    "                    \n",
    "                    if condition == 0:\n",
    "                        if correctness[index] == 1:\n",
    "                            normal_corrects += 1\n",
    "                        normal_count += 1\n",
    "                    if condition == 1:\n",
    "                        if correctness[index] == 1:\n",
    "                            ad_corrects += 1\n",
    "                        ad_count += 1\n",
    "                    index += 1\n",
    "    \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_normal_acc = normal_corrects / normal_count\n",
    "#             print(normal_corrects, normal_count)\n",
    "            epoch_ad_acc = ad_corrects / ad_count\n",
    "#             print(ad_corrects, ad_count)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('Normal Acc: {:.4f} AD Acc: {:.4f}'.format(epoch_normal_acc, epoch_ad_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    elif model_name == \"vgg3d\":\n",
    "        model_ft = VGG3D()\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "print(scratch_model)\n",
    "# scratch_model.apply(weight_init)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.01, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shist = []\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 5.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
