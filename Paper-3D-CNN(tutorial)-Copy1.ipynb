{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1\n",
      "Torchvision Version:  0.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import torch.nn.init as init\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_list = next(os.walk('../Data/Images'))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/Labels.csv\"\n",
    "header = ['Run','AD']\n",
    "Labelsdf = pd.read_csv(path, names=header, usecols=[1,2], skiprows=1, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = dict(zip(Labelsdf.Run, Labelsdf.AD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# process then save the 3d image ndarray as binary\n",
    "# Images of 96, 96, 48 from 256, 176, 256\n",
    "\n",
    "# Crop in the x and y direction (get center/hippocampi area)\n",
    "# [80:176, 40:136, ]\n",
    "\n",
    "img_dir = \"../Data/Images\"\n",
    "dest_dir = \"../Data/Processed\"\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "    print(\"Created ouput directory: \" + dest_dir)\n",
    "\n",
    "    #low_bound = 100\n",
    "    #high_bound = 200\n",
    "    #img_scale = 1\n",
    "    depth_scale = 2\n",
    "\n",
    "    imgs, labels = [], []\n",
    "\n",
    "    for run in runs_list:\n",
    "#         print(run)\n",
    "        run_dir = os.path.join(img_dir, run)\n",
    "        run_imgs = []\n",
    "        for filename in os.listdir(run_dir):\n",
    "            img_slice = cv2.imread(os.path.join(run_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            # if image is square? IF DO THIS, NEED TO ACCOUNT FOR CENTER OFF SET (176 X 240)\n",
    "            if  img_slice is None or img_slice.shape[0] == img_slice.shape[1]:\n",
    "                print(run)\n",
    "                print('1')\n",
    "                break\n",
    "            img_num = int(filename[-7:-4])\n",
    "            if (100 <= img_num < 196):\n",
    "                img_slice = cv2.imread(os.path.join(run_dir, filename), cv2.IMREAD_GRAYSCALE)\n",
    "                #img_slice = cv2.resize(img_slice, (0,0), fx=1/img_scale, fy=1/img_scale, interpolation=cv2.INTER_AREA)\n",
    "                img_slice = img_slice[80:176,40:136]\n",
    "                # normalize pixel intensity to range of 0 and 1\n",
    "                img_slice = img_slice/256\n",
    "                run_imgs.append(img_slice)\n",
    "\n",
    "        if len(run_imgs) == 0 or len(run_imgs) != 96:\n",
    "            print(run)\n",
    "            print('2')\n",
    "            continue\n",
    "\n",
    "        temp_arr = np.array(run_imgs)\n",
    "\n",
    "        final_slices = []\n",
    "        #print(temp_arr.shape[2])\n",
    "\n",
    "        for y in range(temp_arr.shape[2]):\n",
    "            xz_pane = temp_arr[:, :, y]\n",
    "            scaled_xz = cv2.resize(xz_pane, (0, 0), fy=1/depth_scale, fx=1, interpolation=cv2.INTER_AREA)\n",
    "            final_slices.append(scaled_xz)\n",
    "\n",
    "        if not len(final_slices[0]) == 48:\n",
    "            print(run)\n",
    "            print('3')\n",
    "            continue\n",
    "\n",
    "        final_slices = np.dstack(final_slices)\n",
    "        final_slices = np.expand_dims(final_slices, axis=0)\n",
    "        #final_array = torch.from_numpy(np.dstack(final_slices)).float()\n",
    "        if run in labels_dict:\n",
    "            # save as binary\n",
    "            np.save(os.path.join(dest_dir, run), final_slices)\n",
    "            imgs.append(run)\n",
    "            labels.append(labels_dict[run])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    imgs = np.array(imgs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    np.save(os.path.join(dest_dir, 'imgs'), imgs)\n",
    "    np.save(os.path.join(dest_dir, 'labels'), labels)\n",
    "\n",
    "            \n",
    "else:\n",
    "    imgs = np.load(os.path.join(dest_dir, 'imgs.npy'))\n",
    "    labels = np.load(os.path.join(dest_dir, 'labels.npy'))\n",
    "    \n",
    "\n",
    "#     subject_regex = re.compile(\"OAS(?P<order>[0-9]+)\")\n",
    "#     subject = subject_regex.search(run).group(1)\n",
    "    \n",
    "#     if int(subject[-1]) < 6:\n",
    "#         train_data.append(run_tuple)\n",
    "#     elif 6 <= int(subject[-1]) <=7:\n",
    "#         valid_data.append(run_tuple)\n",
    "#     elif 8 <= int(subject[-1]) <= 9:\n",
    "#         test_data.append(run_tuple)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# print(\"Number of data points in train dataset: {}\".format(len(train_data)))\n",
    "# print(\"Number of data points in valid dataset: {}\".format(len(valid_data)))\n",
    "# print(\"Number of data points in test dataset: {}\".format(len(test_data)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg16bn_custom\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 100\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, train_labels = [], []\n",
    "val_imgs, val_labels = [], []\n",
    "test_imgs, test_labels = [], []\n",
    "\n",
    "for idx, ad in np.ndenumerate(labels):\n",
    "    if (idx[0] % 10) < 6:\n",
    "        train_imgs.append(imgs[idx[0]])\n",
    "        train_labels.append(ad)\n",
    "    if ((idx[0] % 10) >= 6) and ((idx[0] % 10) < 8):\n",
    "        val_imgs.append(imgs[idx[0]])\n",
    "        val_labels.append(ad)\n",
    "    if ((idx[0] % 10) >= 8) and ((idx[0] % 10) <= 9):\n",
    "        test_imgs.append(imgs[idx[0]])\n",
    "        test_labels.append(ad)\n",
    "        \n",
    "train_imgs = np.array(train_imgs)\n",
    "train_labels = np.array(train_labels)\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_labels = np.array(val_labels)\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_labels = np.array(test_labels)\n",
    "        \n",
    "ad_imgs, ad_labels = [], []\n",
    "\n",
    "for idx, ad in np.ndenumerate(train_labels):\n",
    "    if ad:\n",
    "        ad_imgs.append(imgs[idx[0]])\n",
    "        ad_labels.append(ad)\n",
    "\n",
    "ad_imgs = np.array(ad_imgs)\n",
    "ad_labels = np.array(ad_labels)\n",
    "\n",
    "normal_ratio = train_imgs.shape[0] // ad_imgs.shape[0]\n",
    "\n",
    "train_imgs = np.concatenate((train_imgs, np.repeat(ad_imgs, normal_ratio - 1, axis=0)), axis=0)\n",
    "train_labels = np.concatenate((train_labels, np.repeat(ad_labels, normal_ratio -1, axis=0)), axis=0)\n",
    "\n",
    "# new_imgs = imgs\n",
    "# new_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T1Dataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = data\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         print(self.data[index])\n",
    "        x = np.load(os.path.join(dest_dir, self.data[index]) + \".npy\")\n",
    "#         print(x.shape)\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = self.target[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = T1Dataset(train_imgs, train_labels)\n",
    "val_dataset = T1Dataset(val_imgs, val_labels)\n",
    "test_dataset = T1Dataset(test_imgs, test_labels)\n",
    "\n",
    "image_datasets = {'train': train_dataset, 'val': val_dataset, 'test': test_dataset}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                   batch_size=batch_size,\n",
    "                                                   pin_memory=True,\n",
    "                                                   shuffle=True, num_workers=0) for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper3DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Paper3DCNN, self).__init__()\n",
    "        self.name = \"Paper3DCNN\"\n",
    "        # ?x1x96x96x48 to ?x32x96x96x48\n",
    "        # kernel size is always 3 or 1 in vgg, stride is 1, padding is s.t. the resolution preserved after convolution\n",
    "        # padding is 1 pixel for 3x3 convolution\n",
    "        # max pooling 2, 2\n",
    "        \n",
    "        # first block\n",
    "        self.conv1_1 = nn.Conv3d(1, 32, padding=1, kernel_size=3, bias=True)\n",
    "        # max pool\n",
    "        \n",
    "        # second block\n",
    "        self.conv2_1 = nn.Conv3d(32, 64, padding=1, kernel_size=3, bias=True)\n",
    "        # max pool\n",
    "        \n",
    "        # third block\n",
    "        self.conv3_1 = nn.Conv3d(64, 128, padding=1, kernel_size=3, bias=True)\n",
    "        self.conv3_2 = nn.Conv3d(128, 128, padding=1, kernel_size=3, bias=True)\n",
    "        # max pool\n",
    "        \n",
    "        # fourth block\n",
    "        self.conv4_1 = nn.Conv3d(128, 256, padding=1, kernel_size=3, bias=True)\n",
    "        self.conv4_2 = nn.Conv3d(256, 256, padding=1, kernel_size=3, bias=True)\n",
    "        # max pool\n",
    "        \n",
    "        # fifth block\n",
    "        self.conv5_1 = nn.Conv3d(256, 256, padding=1, kernel_size=3, bias=True)\n",
    "        self.conv5_2 = nn.Conv3d(256, 256, padding=1, kernel_size=3, bias=True)\n",
    "        # max pool 2\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool3d(2, 2)\n",
    "        self.maxpool2 = nn.MaxPool3d(kernel_size=(2,2,2), padding=(1,0,0), stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4608, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #residual = x\n",
    "        \n",
    "        # first block\n",
    "        out = self.conv1_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        # second block\n",
    "        out = self.conv2_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        # third block\n",
    "        out = self.conv3_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        # fourth block\n",
    "        out = self.conv4_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv4_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        # fourth block\n",
    "        out = self.conv5_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv5_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        # if self.downsample is not None:\n",
    "        #     residual = self.downsample(x)\n",
    "\n",
    "        #out += residual\n",
    "        out = out.view(-1, 4608)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model \n",
    "    '''\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "#         self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 3 * 3, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 2),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 3, 3))\n",
    "        \n",
    "        self.features3d = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #256\n",
    "            \n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #128\n",
    "            \n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #64\n",
    "            \n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #32\n",
    "            \n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2)), #16\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = torch.reshape(x, (-1, 48, 96, 96))\n",
    "#         x = self.features(x)\n",
    "#         x = torch.unsqueeze(x, 1)\n",
    "        x = self.features3d(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         exit()\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "#         print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 48\n",
    "#     in_channels_3d = 1\n",
    "#     dimension = '2d' \n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "#         if dimension = '3d':\n",
    "#             if v == 'M':\n",
    "#                 layers += [nn.MaxPool3d(kernel_size=(2, 1, 1))]\n",
    "#             else:\n",
    "#                 conv3d = nn.Conv3d(in_channels_3d, v, kernel_size=3, padding=1)\n",
    "#                 if batch_norm:\n",
    "#                     layers += [conv3d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "#                 else:\n",
    "#                     layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "#                 in_channels_3d = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def vgg11():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']))\n",
    "\n",
    "\n",
    "def vgg11_bn():\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg13():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']))\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            normal_corrects, ad_corrects = 0, 0\n",
    "            normal_count, ad_count = 0, 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "#                 print(inputs.shape)\n",
    "#                 new1 = torch.reshape(inputs, (-1, 96, 96))\n",
    "#                 print(new1.shape)\n",
    "#                 new2 = torch.unsqueeze(new1, 1)\n",
    "#                 print(new2.shape)\n",
    "#                 exit()\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "#                         print(outputs, labels)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "#                 print(preds)\n",
    "#                 print(labels.data)\n",
    "                correctness = preds == labels.data\n",
    "                running_corrects += torch.sum(correctness)\n",
    "                \n",
    "                \n",
    "                index = 0\n",
    "                for condition in labels.data:\n",
    "                    \n",
    "                    if condition == 0:\n",
    "                        if correctness[index] == 1:\n",
    "                            normal_corrects += 1\n",
    "                        normal_count += 1\n",
    "                    if condition == 1:\n",
    "                        if correctness[index] == 1:\n",
    "                            ad_corrects += 1\n",
    "                        ad_count += 1\n",
    "                    index += 1\n",
    "                \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_normal_acc = normal_corrects / normal_count\n",
    "#             print(normal_corrects, normal_count)\n",
    "            epoch_ad_acc = ad_corrects / ad_count\n",
    "#             print(ad_corrects, ad_count)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('Normal Acc: {:.4f} AD Acc: {:.4f}'.format(epoch_normal_acc, epoch_ad_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weight_init(m):\n",
    "#     '''\n",
    "#     Usage:\n",
    "#         model = Model()\n",
    "#         model.apply(weight_init)\n",
    "#     '''\n",
    "#     if isinstance(m, nn.Conv1d):\n",
    "#         init.normal_(m.weight.data)\n",
    "#         if m.bias is not None:\n",
    "#             init.normal_(m.bias.data)\n",
    "#     elif isinstance(m, nn.Conv2d):\n",
    "#         init.xavier_normal_(m.weight.data)\n",
    "#         if m.bias is not None:\n",
    "#             init.normal_(m.bias.data)\n",
    "#     elif isinstance(m, nn.Conv3d):\n",
    "#         init.xavier_normal_(m.weight.data)\n",
    "#         if m.bias is not None:\n",
    "#             init.normal_(m.bias.data)\n",
    "#     elif isinstance(m, nn.ConvTranspose1d):\n",
    "#         init.normal_(m.weight.data)\n",
    "#         if m.bias is not None:\n",
    "#             init.normal_(m.bias.data)\n",
    "#     elif isinstance(m, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(m.weight.data)\n",
    "#         if m.bias is not None:\n",
    "#             init.normal_(m.bias.data)\n",
    "#     elif isinstance(m, nn.ConvTranspose3d):\n",
    "#         init.xavier_normal_(m.weight.data)\n",
    "#         if m.bias is not None:\n",
    "#             init.normal_(m.bias.data)\n",
    "#     elif isinstance(m, nn.BatchNorm1d):\n",
    "#         init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "#         init.constant_(m.bias.data, 0)\n",
    "#     elif isinstance(m, nn.BatchNorm2d):\n",
    "#         init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "#         init.constant_(m.bias.data, 0)\n",
    "#     elif isinstance(m, nn.BatchNorm3d):\n",
    "#         init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "#         init.constant_(m.bias.data, 0)\n",
    "#     elif isinstance(m, nn.Linear):\n",
    "#         init.xavier_normal_(m.weight.data)\n",
    "#         init.normal_(m.bias.data)\n",
    "#     elif isinstance(m, nn.LSTM):\n",
    "#         for param in m.parameters():\n",
    "#             if len(param.shape) >= 2:\n",
    "#                 init.orthogonal_(param.data)\n",
    "#             else:\n",
    "#                 init.normal_(param.data)\n",
    "#     elif isinstance(m, nn.LSTMCell):\n",
    "#         for param in m.parameters():\n",
    "#             if len(param.shape) >= 2:\n",
    "#                 init.orthogonal_(param.data)\n",
    "#             else:\n",
    "#                 init.normal_(param.data)\n",
    "#     elif isinstance(m, nn.GRU):\n",
    "#         for param in m.parameters():\n",
    "#             if len(param.shape) >= 2:\n",
    "#                 init.orthogonal_(param.data)\n",
    "#             else:\n",
    "#                 init.normal_(param.data)\n",
    "#     elif isinstance(m, nn.GRUCell):\n",
    "#         for param in m.parameters():\n",
    "#             if len(param.shape) >= 2:\n",
    "#                 init.orthogonal_(param.data)\n",
    "#             else:\n",
    "#                 init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    elif model_name == \"Paper3DCNN\":\n",
    "        model_ft = Paper3DCNN()\n",
    "        \n",
    "    elif model_name == \"vgg16bn_custom\":\n",
    "        model_ft = vgg16_bn()\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# # Initialize the model for this run\n",
    "# model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# # Print the model we just instantiated\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=4608, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 3, 3))\n",
      "  (features3d): Sequential(\n",
      "    (0): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (8): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (11): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace)\n",
      "    (13): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (15): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace)\n",
      "    (17): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (18): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace)\n",
      "    (20): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (22): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU(inplace)\n",
      "    (24): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (25): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (29): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): ReLU(inplace)\n",
      "    (31): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (32): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (33): ReLU(inplace)\n",
      "    (34): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.7983 Acc: 0.4967\n",
      "Normal Acc: 0.3493 AD Acc: 0.6320\n",
      "val Loss: 0.7344 Acc: 0.2635\n",
      "Normal Acc: 0.1558 AD Acc: 0.8043\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.6967 Acc: 0.5113\n",
      "Normal Acc: 0.2757 AD Acc: 0.7277\n",
      "val Loss: 0.6571 Acc: 0.8159\n",
      "Normal Acc: 0.9675 AD Acc: 0.0543\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.6957 Acc: 0.5146\n",
      "Normal Acc: 0.3292 AD Acc: 0.6849\n",
      "val Loss: 0.6918 Acc: 0.3430\n",
      "Normal Acc: 0.2424 AD Acc: 0.8478\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.6957 Acc: 0.5126\n",
      "Normal Acc: 0.2993 AD Acc: 0.7085\n",
      "val Loss: 0.7675 Acc: 0.1661\n",
      "Normal Acc: 0.0000 AD Acc: 1.0000\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.6949 Acc: 0.5233\n",
      "Normal Acc: 0.2243 AD Acc: 0.7978\n",
      "val Loss: 0.6967 Acc: 0.3574\n",
      "Normal Acc: 0.2857 AD Acc: 0.7174\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.6946 Acc: 0.5206\n",
      "Normal Acc: 0.3354 AD Acc: 0.6907\n",
      "val Loss: 0.7550 Acc: 0.1661\n",
      "Normal Acc: 0.0000 AD Acc: 1.0000\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.6947 Acc: 0.5076\n",
      "Normal Acc: 0.2847 AD Acc: 0.7124\n",
      "val Loss: 0.7611 Acc: 0.1661\n",
      "Normal Acc: 0.0000 AD Acc: 1.0000\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.6950 Acc: 0.5229\n",
      "Normal Acc: 0.3187 AD Acc: 0.7105\n",
      "val Loss: 0.6691 Acc: 0.6444\n",
      "Normal Acc: 0.6883 AD Acc: 0.4239\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.6935 Acc: 0.5326\n",
      "Normal Acc: 0.3611 AD Acc: 0.6901\n",
      "val Loss: 0.8048 Acc: 0.1661\n",
      "Normal Acc: 0.0000 AD Acc: 1.0000\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.6959 Acc: 0.5203\n",
      "Normal Acc: 0.2938 AD Acc: 0.7283\n",
      "val Loss: 0.8180 Acc: 0.1661\n",
      "Normal Acc: 0.0000 AD Acc: 1.0000\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.6867 Acc: 0.5362\n",
      "Normal Acc: 0.3979 AD Acc: 0.6633\n",
      "val Loss: 0.6845 Acc: 0.8321\n",
      "Normal Acc: 0.9913 AD Acc: 0.0326\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.6955 Acc: 0.5160\n",
      "Normal Acc: 0.3368 AD Acc: 0.6805\n",
      "val Loss: 0.7699 Acc: 0.1661\n",
      "Normal Acc: 0.0000 AD Acc: 1.0000\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.6939 Acc: 0.5130\n",
      "Normal Acc: 0.2375 AD Acc: 0.7659\n",
      "val Loss: 0.7051 Acc: 0.2040\n",
      "Normal Acc: 0.0671 AD Acc: 0.8913\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.6924 Acc: 0.5163\n",
      "Normal Acc: 0.3958 AD Acc: 0.6269\n",
      "val Loss: 0.6952 Acc: 0.5758\n",
      "Normal Acc: 0.5584 AD Acc: 0.6630\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.6864 Acc: 0.5249\n",
      "Normal Acc: 0.4444 AD Acc: 0.5989\n",
      "val Loss: 0.7278 Acc: 0.3718\n",
      "Normal Acc: 0.2814 AD Acc: 0.8261\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.6860 Acc: 0.5565\n",
      "Normal Acc: 0.5437 AD Acc: 0.5682\n",
      "val Loss: 0.7030 Acc: 0.6282\n",
      "Normal Acc: 0.5801 AD Acc: 0.8696\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.6774 Acc: 0.5622\n",
      "Normal Acc: 0.6250 AD Acc: 0.5045\n",
      "val Loss: 0.5952 Acc: 0.8375\n",
      "Normal Acc: 0.9935 AD Acc: 0.0543\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.6779 Acc: 0.5422\n",
      "Normal Acc: 0.6236 AD Acc: 0.4675\n",
      "val Loss: 0.8171 Acc: 0.4639\n",
      "Normal Acc: 0.3680 AD Acc: 0.9457\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.6758 Acc: 0.5439\n",
      "Normal Acc: 0.5368 AD Acc: 0.5504\n",
      "val Loss: 0.6258 Acc: 0.8448\n",
      "Normal Acc: 0.9913 AD Acc: 0.1087\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.6716 Acc: 0.5628\n",
      "Normal Acc: 0.6736 AD Acc: 0.4611\n",
      "val Loss: 0.6943 Acc: 0.7058\n",
      "Normal Acc: 0.6775 AD Acc: 0.8478\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.6639 Acc: 0.5718\n",
      "Normal Acc: 0.7049 AD Acc: 0.4496\n",
      "val Loss: 1.1562 Acc: 0.2852\n",
      "Normal Acc: 0.1429 AD Acc: 1.0000\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.6678 Acc: 0.5685\n",
      "Normal Acc: 0.6549 AD Acc: 0.4892\n",
      "val Loss: 0.6924 Acc: 0.7220\n",
      "Normal Acc: 0.7208 AD Acc: 0.7283\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.6588 Acc: 0.5824\n",
      "Normal Acc: 0.6986 AD Acc: 0.4758\n",
      "val Loss: 0.5454 Acc: 0.8538\n",
      "Normal Acc: 0.9610 AD Acc: 0.3152\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.6553 Acc: 0.6014\n",
      "Normal Acc: 0.7049 AD Acc: 0.5064\n",
      "val Loss: 0.6186 Acc: 0.8267\n",
      "Normal Acc: 0.8680 AD Acc: 0.6196\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.6514 Acc: 0.6227\n",
      "Normal Acc: 0.7299 AD Acc: 0.5242\n",
      "val Loss: 0.6405 Acc: 0.7635\n",
      "Normal Acc: 0.7835 AD Acc: 0.6630\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.6437 Acc: 0.6210\n",
      "Normal Acc: 0.7465 AD Acc: 0.5057\n",
      "val Loss: 0.7470 Acc: 0.4513\n",
      "Normal Acc: 0.3636 AD Acc: 0.8913\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6336\n",
      "Normal Acc: 0.7174 AD Acc: 0.5568\n",
      "val Loss: 0.9662 Acc: 0.4549\n",
      "Normal Acc: 0.3571 AD Acc: 0.9457\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.6335 Acc: 0.6423\n",
      "Normal Acc: 0.7264 AD Acc: 0.5651\n",
      "val Loss: 0.6166 Acc: 0.7401\n",
      "Normal Acc: 0.7468 AD Acc: 0.7065\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.6236 Acc: 0.6572\n",
      "Normal Acc: 0.7264 AD Acc: 0.5938\n",
      "val Loss: 0.5911 Acc: 0.7653\n",
      "Normal Acc: 0.8225 AD Acc: 0.4783\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.6137 Acc: 0.6592\n",
      "Normal Acc: 0.7250 AD Acc: 0.5989\n",
      "val Loss: 0.5471 Acc: 0.8430\n",
      "Normal Acc: 0.9610 AD Acc: 0.2500\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.5985 Acc: 0.6842\n",
      "Normal Acc: 0.7083 AD Acc: 0.6620\n",
      "val Loss: 0.7863 Acc: 0.5235\n",
      "Normal Acc: 0.4567 AD Acc: 0.8587\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.5896 Acc: 0.6938\n",
      "Normal Acc: 0.6917 AD Acc: 0.6958\n",
      "val Loss: 0.8018 Acc: 0.4675\n",
      "Normal Acc: 0.4069 AD Acc: 0.7717\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.5651 Acc: 0.7134\n",
      "Normal Acc: 0.7035 AD Acc: 0.7226\n",
      "val Loss: 0.6017 Acc: 0.6733\n",
      "Normal Acc: 0.7186 AD Acc: 0.4457\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.5423 Acc: 0.7364\n",
      "Normal Acc: 0.7035 AD Acc: 0.7666\n",
      "val Loss: 0.6312 Acc: 0.7184\n",
      "Normal Acc: 0.7273 AD Acc: 0.6739\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.5158 Acc: 0.7530\n",
      "Normal Acc: 0.7215 AD Acc: 0.7819\n",
      "val Loss: 1.0410 Acc: 0.5072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Acc: 0.4329 AD Acc: 0.8804\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.4992 Acc: 0.7646\n",
      "Normal Acc: 0.7139 AD Acc: 0.8112\n",
      "val Loss: 0.6287 Acc: 0.6318\n",
      "Normal Acc: 0.6645 AD Acc: 0.4674\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.4689 Acc: 0.7879\n",
      "Normal Acc: 0.7153 AD Acc: 0.8546\n",
      "val Loss: 0.8237 Acc: 0.4170\n",
      "Normal Acc: 0.3550 AD Acc: 0.7283\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.4578 Acc: 0.8002\n",
      "Normal Acc: 0.7465 AD Acc: 0.8495\n",
      "val Loss: 0.5463 Acc: 0.7419\n",
      "Normal Acc: 0.7814 AD Acc: 0.5435\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.4204 Acc: 0.8185\n",
      "Normal Acc: 0.7500 AD Acc: 0.8814\n",
      "val Loss: 0.7646 Acc: 0.5866\n",
      "Normal Acc: 0.5519 AD Acc: 0.7609\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.3933 Acc: 0.8348\n",
      "Normal Acc: 0.7722 AD Acc: 0.8922\n",
      "val Loss: 0.6681 Acc: 0.7076\n",
      "Normal Acc: 0.7641 AD Acc: 0.4239\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.3875 Acc: 0.8434\n",
      "Normal Acc: 0.7799 AD Acc: 0.9018\n",
      "val Loss: 0.4698 Acc: 0.7581\n",
      "Normal Acc: 0.7900 AD Acc: 0.5978\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.3736 Acc: 0.8467\n",
      "Normal Acc: 0.7833 AD Acc: 0.9050\n",
      "val Loss: 0.6959 Acc: 0.6769\n",
      "Normal Acc: 0.6558 AD Acc: 0.7826\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.3328 Acc: 0.8737\n",
      "Normal Acc: 0.8056 AD Acc: 0.9362\n",
      "val Loss: 0.7733 Acc: 0.6354\n",
      "Normal Acc: 0.6147 AD Acc: 0.7391\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.3153 Acc: 0.8813\n",
      "Normal Acc: 0.8236 AD Acc: 0.9343\n",
      "val Loss: 0.5964 Acc: 0.7437\n",
      "Normal Acc: 0.7814 AD Acc: 0.5543\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.3031 Acc: 0.8840\n",
      "Normal Acc: 0.8229 AD Acc: 0.9401\n",
      "val Loss: 0.6333 Acc: 0.7256\n",
      "Normal Acc: 0.7403 AD Acc: 0.6522\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.2848 Acc: 0.8949\n",
      "Normal Acc: 0.8361 AD Acc: 0.9490\n",
      "val Loss: 0.8181 Acc: 0.6390\n",
      "Normal Acc: 0.5996 AD Acc: 0.8370\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.2867 Acc: 0.8949\n",
      "Normal Acc: 0.8389 AD Acc: 0.9464\n",
      "val Loss: 0.7514 Acc: 0.6769\n",
      "Normal Acc: 0.6623 AD Acc: 0.7500\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.2838 Acc: 0.8943\n",
      "Normal Acc: 0.8306 AD Acc: 0.9528\n",
      "val Loss: 0.6004 Acc: 0.7942\n",
      "Normal Acc: 0.8636 AD Acc: 0.4457\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.2442 Acc: 0.9159\n",
      "Normal Acc: 0.8590 AD Acc: 0.9681\n",
      "val Loss: 0.6293 Acc: 0.7491\n",
      "Normal Acc: 0.7727 AD Acc: 0.6304\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.2395 Acc: 0.9159\n",
      "Normal Acc: 0.8611 AD Acc: 0.9662\n",
      "val Loss: 0.7267 Acc: 0.8051\n",
      "Normal Acc: 0.8658 AD Acc: 0.5000\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.2535 Acc: 0.9056\n",
      "Normal Acc: 0.8465 AD Acc: 0.9598\n",
      "val Loss: 0.7150 Acc: 0.7726\n",
      "Normal Acc: 0.8290 AD Acc: 0.4891\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.2265 Acc: 0.9245\n",
      "Normal Acc: 0.8715 AD Acc: 0.9732\n",
      "val Loss: 0.7464 Acc: 0.7798\n",
      "Normal Acc: 0.8485 AD Acc: 0.4348\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ed866d248cc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscratch_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscratch_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mscratch_criterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscratch_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscratch_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscratch_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscratch_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"inception\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-0cc36ad15e61>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;31m# statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;31m#                 print(preds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m#                 print(labels.data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "print(scratch_model)\n",
    "# scratch_model.apply(weight_init)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.01, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scratch_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cd9ceb8c3dea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mshist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mshist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscratch_hist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation Accuracy vs. Number of Training Epochs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scratch_hist' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "shist = []\n",
    "\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.tensor([[[[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]]], \n",
    "#                  [[[[12, 13], [14, 15]], [[16, 17], [18, 19]], [[20, 21], [22, 23]]]]])\n",
    "# print(t)\n",
    "# print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.reshape(t, (-1, 2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.reshape(t, (-1, 2, 2)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [32, 512, 3, 3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
